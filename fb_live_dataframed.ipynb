{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A DataFramed code along special: the favorite techniques of the experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have recently launched a [new data science podcast called DataFramed](https://www.datacamp.com/community/podcast), in which I speak with experts and thought leaders from academia and industry about what data science looks like in practice and how it's changing society. I often ask my guests what one of their data sciencey techniques is. Today, I'll take you through a bunch of them!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/DataFramed 1400x1400.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "We'll look at \n",
    "\n",
    "* Scatter plots\n",
    "* Decision trees\n",
    "* Linear regression\n",
    "* Using log axes\n",
    "* Logistic regression\n",
    "* PCA\n",
    "\n",
    "You can subscribe to DataFramed [on iTunes here](https://itunes.apple.com/us/podcast/dataframed/id1336150688) and on [the Google play store here](https://play.google.com/music/listen?u=0#/ps/Idltnsiq2bvzpfzn5tni3iixyta).\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store].(https://itunes.apple.com/us/podcast/dataframed/id1336150688)\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# 1 Scatter plots\n",
    "\n",
    "Roger Peng appeared on [this week's episode of DataFramed](https://www.datacamp.com/community/podcast/data-science-environment-moocs). Roger is a Professor in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health, co-director of the Johns Hopkins Data Science Lab and co-founder of the Johns Hopkins Data Science Specialization. Roger is also a well-seasoned podcaster on Not so Standard deviations and the Effort Report. In this episode, we talked about data science, it's role in researching the environment and air pollution, massive open online courses for democratizing data science and much more.\n",
    "\n",
    "In Roger's words,\n",
    "\n",
    "> ** Frankly, my favorite tool is just simply a scatter plot. I think plotting is so revealing. It's not something that, frankly, I see a lot done. I think the reason why, I thought about why this is the case, and I think the reason is because it's one of those tools that really instills trust in the people who receive the plot. Because they feel like they can see the data, they feel like they can understand if you have a model that's overlaid they know how the data goes into the model. They can reason about the data and I think it's one of the really critical things for building trust.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's now build some scatter plots to see their power. First you'll import some required packages, import your data and check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and check out several rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the column names, types and how many entries are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check out summary statistics of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** what is the data? Check it out <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why are we interested in this data?\n",
    "    \n",
    "**One answer:** To predict diagnosis (benign or malignant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:**\n",
    "\n",
    "* We've imported the UCI breast cancer dataset and explored it (but not visually).\n",
    "* Next up: we'll see the power of scatter plots and why Roger Peng loves them so much.\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store](https://itunes.apple.com/us/podcast/dataframed/id1336150688).\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now it's time to build some plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of 1st two features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot colored by 'target'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot colored by 'target' with linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now check out some other features that we think may be related, such as 'mean radius' and 'mean perimeter'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check out 'mean radius' vs 'mean perimeter':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You're now going to build a pairplot of this dataset (this will contain all possible scatter plots of features, with histograms along the diagonal). But first you'll subset the data to return the first four and the final column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now it is time to build you pairplot, using `seaborn`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:**\n",
    "\n",
    "* We've imported the UCI breast cancer dataset and explored it (but not visually).\n",
    "* We have seen the power of scatter plots and why Roger Peng loves them so much.\n",
    "* Next up: the machine learning superpower of decision trees.\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store](https://itunes.apple.com/us/podcast/dataframed/id1336150688).\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# 2 Decision trees for prediction\n",
    "\n",
    "In [episode 2 of DataFramed](https://www.datacamp.com/community/podcast/data-science-telecommunications), I spoke with Chris Volinksy, Assistant Vice President for Big Data Research at AT&T Labs, and all around top bloke.\n",
    "\n",
    "Chris Volinsky:\n",
    "> **I'm always amazed at the power of some of the old school techniques. Good old fashioned linear regression is still a really powerful and interpretable, and tried and true technique. It's not always appropriate, but often works well. Decision trees are another old school technique, I'm always amazed at how well they work. But, you know, one thing I always find really powerful are well done, well-thought out data visualizations. And, you know, I'm a big fan of the type of data visualization that I see in media companies.**\n",
    "\n",
    "You've already done some datavis, you'll soon do some linear regression. So now you're going to build a decision tree (classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So: what is a decision tree classifier? It is a tree that allows you to classify data points (aka predict target variables, e.g. benign or malignant tumor) based on feature variables (such as geometric measurements of tumors). For example,\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/bc.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "**Why do our podcast guests like these?** Because they are interpretable! Another way of saying this is\n",
    "\n",
    "> An interpretable model is one whose predictions you can explain. \n",
    "\n",
    "-- Mike Lee Williams, Research Engineer, Cloudera Fast Forward Labs (check out [this segment](https://soundcloud.com/dataframed/9-data-science-and-online-experiments-at-etsy#t=17:10))\n",
    "\n",
    "* You first **fit** such a model to your training data, which means deciding (based on the training data) which decisions will split at each branching point in the tree. E.g. that the first branch is on the feature 'mean area' and that 'mean area' less than 696.25 results in a prediction of 'benign'. \n",
    "\n",
    "**Note** that it's actually the Gini coefficient which is used to make these decisions. At this point, you won't delve deeper into these stuff.\n",
    "\n",
    "So let's now build a decision tree classifier. First up, create `numpy` arrays `X` and `y` that contain your features and your target, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You'll want to fit (or train) your model on a subset of the data, called the _training set_. * You'll then test it on the another set, the _test set_. Testing it means predicting and seeing how good the predictions are. \n",
    "* You'll use a metric called _accuracy_, which is the fraction of correct predictions. \n",
    "* **Now** split your data in training/test sets using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you get to build your decision tree classifier. First create such a model with `max_depth=2` and then fit it your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For fun, compute the score on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize your decision tree using `graphviz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=df_sub.drop('target', axis=1).columns,  \n",
    "                         class_names=['malignant', 'benign'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:**\n",
    "\n",
    "* We've imported the UCI breast cancer dataset and explored it (but not visually).\n",
    "* We have seen the power of scatter plots and why Roger Peng loves them so much.\n",
    "* We've used the machine learning superpower of decision trees.\n",
    "* Next up: linear regression and interpretability.\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store](https://itunes.apple.com/us/podcast/dataframed/id1336150688).\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# 3 Linear regression\n",
    "\n",
    "We already saw that Chris Volinsky was a huge fan of linear regression:\n",
    "> **But, I'm always amazed at the power of some of the old school techniques. Good old fashioned linear regression is still a really powerful and interpretable, and tried and true technique.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tumor prediction task was a _classification task_, you were trying to classify _tumors_. \n",
    "\n",
    "The other well-known prediction task is called a _regression task_, in which you're trying to predict a numeric quantity, such as the life expectancy in a given nation. \n",
    "\n",
    "Let's import some [Gapminder](https://www.gapminder.org/) data to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and check out first rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the column names, types and how many entries are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to use a linear regression model to try to predict the life expectancy in a given country, based on its fertility rate. But first, make a scatter plot ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model fits a straight **line** to the data:\n",
    "\n",
    "$$y = a_0 + a_1x.$$\n",
    "\n",
    "This is once again an interpretable model as it tells us\n",
    "\n",
    "* a 1-unit increase in $x$ leads to an $a_1$ increase in $y$.\n",
    "\n",
    "Lets now see this in action. You'll fit the model to the entire data set and visualize the regression (fitting the model determines the parameters $a_i$ in the above equation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data into feature and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression\n",
    "\n",
    "\n",
    "# Create the regressor: reg\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "\n",
    "\n",
    "# Plot scatter plot of data\n",
    "\n",
    "\n",
    "# Create the prediction space\n",
    "prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n",
    "\n",
    "# Compute predictions over the prediction space: y_pred\n",
    "\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3);\n",
    "\n",
    "# Print R^2 \n",
    "print(reg.score(X_fertility, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Question:** Looking at the above figure, what would you expect the regression coefficient of interest to be? Now you're going to print the regression coefficient from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression coefficient(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note:** You'll genereally want to normalize your data before using regression models and you may want to used a penalized regression such as lasso or ridge regression. See our [Supervised Learning with scikit-learn course](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn) for more on these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You'll now do the same using a two parameter model ('fertility', 'GDP'):\n",
    "\n",
    "$$y = a_0 + a_1x_1 + a_2x_2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from `df_gm`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor: reg\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "\n",
    "\n",
    "# Print R^2 \n",
    "print(reg.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression coefficient(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Interpret** the above regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But hold up. We didn't plot 'GDP'. What does it look like against 'life'. Plot it now to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** GDP is pretty bunched up between 0 and 40K but there are also values > 100,000. Are there plotting techniques to deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Plotting with log axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [episode 6 of DataFramed](https://www.datacamp.com/community/podcast/citizen-data-science), I interviewed David Robinson, Chief Data Scientist at DataCamp, about _Citizen Data Science_. Dave's favorite technique is using log axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **So this is a simple technique, but it's one that I think is really underrated and is really kind of one of my favorites. It's learn to put something on a log scale. That is, take it from numbers that go one, two, three, four, five, six and if you can just instead have a scale that goes 1, 10, 100, 1,000. So that's really important when grafting because so many sets of numbers that we work with in the real world exist on scales that are much larger. That are these multiple different orders of magnitude.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot 'life' vs 'GDP' with a log axis for 'GDP':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:**\n",
    "\n",
    "* We've imported the UCI breast cancer dataset and explored it (but not visually).\n",
    "* We have seen the power of scatter plots and why Roger Peng loves them so much.\n",
    "* We've used the machine learning superpower of decision trees.\n",
    "* We've used linear regression and and explored its interpretability.\n",
    "* We've used log axes to make our plots easier to read.\n",
    "* Next up: logistic regression for classification challenges.\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store](https://itunes.apple.com/us/podcast/dataframed/id1336150688).\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# 4 Logistic regression\n",
    "\n",
    "In [episode 3 of DataFramed](https://www.datacamp.com/community/podcast/data-science-machine-learning-online-advertising), I interviewed Claudia Perlich, Chief Scientist at Dstillery, where she led the machine learning efforts that help target consumers and derive insights for marketers. We spoke about the role of data science in the online advertising world, the predictability of humans, how Claudia's team built real time bidding algorithms and detected bots online, along with the ethical implications of all of these evolving concepts.\n",
    "\n",
    "> **Today I really value the simplicity and elegance and also transparency that you can get from linear models like logistic regression ... because it's so much easier to look under the hood and understand what might be going on there. It really has become my go to tool over the last I would say 10, 15 years. In fact, I won all of my data mining competitions using some form of a logistic model.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's clarify something: logistic regression is a linear classification algorithm. In this section, you'll use a logistic regression model to build classification predictions for the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does logistic regression work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "Logistic regression essentially takes a linear combination of the features \n",
    "\n",
    "$$t = a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n.$$\n",
    "\n",
    "Then transforms $t$ into\n",
    "\n",
    "$$p = \\frac{1}{1+e^{-t}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets now visualize this transformation $t \\to p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-8,8,100)\n",
    "p = ____\n",
    "plt.plot(t,p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** $p$ is the estimated probability that, for example, the tumor is malignant.\n",
    "\n",
    "**Note:** fitting the model to the data determines the coefficients $a_i$.\n",
    "\n",
    "If $p>0.5$, we classify the target as 1 (malignant), otherwise as 0 (benign).\n",
    "\n",
    "### How is this model interpretable\n",
    "\n",
    "Well, rearranging the above equations yield\n",
    "\n",
    "$$a_0 + a_1x_1 + a_2x_2 + \\ldots + a_nx_n = t = \\text{log}(\\frac{p}{1-p})=\\text{logit}(p)$$\n",
    "\n",
    "And $\\frac{p}{1-p}$ is called the odds ratio: this is the probability of the tumor being malignant over the probability of the tumor being benign.\n",
    "\n",
    "So: increasing $x_1$ by 1 unit will increase the odds ratio $\\frac{p}{1-p}$ by $\\text{exp}(a_1)$ units. It is in this way that logistic regression is interpretable. It's now time to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out 1st several rows of data for reacquaintance purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features/target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build logistic regression model, fit to training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print coefficients of logreg model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [this here](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/):\n",
    "\n",
    "* a 1-unit increase in 'mean texture' will result in logit(p) decreasing by 0.16. Thus the odds ratio decreases by exp(0.16) = 1.17 or changes by a factor of 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp(0.16))\n",
    "print(1/np.exp(0.16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:**\n",
    "\n",
    "* We've imported the UCI breast cancer dataset and explored it (but not visually).\n",
    "* We have seen the power of scatter plots and why Roger Peng loves them so much.\n",
    "* We've used the machine learning superpower of decision trees.\n",
    "* We've used linear regression and and explored its interpretability.\n",
    "* We've used log axes to make our plots easier to read.\n",
    "* We've seen the power of logistic regression.\n",
    "* Next up: PCA as the swiss army knife of machine learning.\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store](https://itunes.apple.com/us/podcast/dataframed/id1336150688).\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# 5 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [episode 8 of DataFramed](https://www.datacamp.com/community/podcast/data-science-astronomy), I chatted with Jake VanderPlas, a data science fellow at the University of Washington's eScience Institute, where his work focuses on data-intensive physical science research in an interdisciplinary setting. In the Python world, Jake is the author of the Python Data Science Handbook, and is active in maintaining and/or contributing to several well-known Python scientific computing packages, including Scikit-learn, Scipy, Matplotlib, Astropy, Altair, and others.\n",
    "\n",
    "> My all-time favorite in machine learning is **principal component analysis**. I just think it’s like a Swiss army knife, you can do anything with it...When I was a grad student I quickly found that whenever I was going to my meeting with my thesis advisor and I had new data set or something to look at, the first question that he was going to ask me was, \"Well, did you do PCA? \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is an example of dimensionality reduction and a favorite way to do it for many working data scientists. It's important as many datasets have way too many features to put into a scalable machine learning pipeline (for example) and it helps you to reduce the dimensionality of your data while retaining as much information as possible. Note: in essence, it's a form of compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot 'mean radius' against 'mean perimeter':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='mean radius', y='mean perimeter', c='target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** why would you want to compress this data, that is, reduce to a lower-dimensional space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is the following: if features are correlated as they are above, we may have enough information if we throw one of them away.\n",
    "\n",
    "* The first step of PCA is to decorrelate your data and this corresponds to a linear transformation of the vector space your data lie in;\n",
    "* The second step is the actual dimension reduction; what is really happening is that your decorrelation step (the first step above) transforms the features into new and uncorrelated features; this second step then chooses the features that contain most of the information about the data (we'll formalize this soon enough).\n",
    "\n",
    "Visualize the PCA transformation that preserves number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original breast cancer data into features/target\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = ____\n",
    "ar_tot = ____\n",
    "\n",
    "# Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "model_tot = ____\n",
    "transformed = ____\n",
    "print(transformed.shape)\n",
    "plt.scatter(transformed[:,0], transformed[:,1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot explained total variance of principal components against number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How much variance is contained in the 1st principal component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you're going to have some real fun by doing PCA before a logistic regression and seeing how many components you need to use to get the best model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original breast cancer data into features/target\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test/train set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline of PCA w/ 20 components and a logistic regression\n",
    "# NOTE: You should also scale your data; this will be an exercise for those\n",
    "# eager ones out there\n",
    "from sklearn.pipeline import Pipeline\n",
    "pca = ____\n",
    "pipe = ____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you're going to build a PCA/logreg pipeline for 1 component, 2 components and so on up to 30 components. You'll then plot accuracy as a function of the number of components used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:**\n",
    "\n",
    "* We've imported the UCI breast cancer dataset and explored it (but not visually).\n",
    "* We have seen the power of scatter plots and why Roger Peng loves them so much.\n",
    "* We've used the machine learning superpower of decision trees.\n",
    "* We've used linear regression and and explored its interpretability.\n",
    "* We've used log axes to make our plots easier to read.\n",
    "* We've seen the power of logistic regression.\n",
    "* We've checked out PCA, the swiss army knife of machine learning.\n",
    "\n",
    "### Give-away\n",
    "\n",
    "We're also having a give-away for those who write iTunes reviews for us! 5 lucky randomly selected reviewers will receive DataCamp swag: we've got sweatshirts, pens, stickers, you name it aaaaaaand one of those 5 will be selected to interview me in one of our podcast segments!\n",
    "\n",
    "**What do you need to do?**\n",
    "\n",
    "* Write a review of DataFramed (a positive one!) in [the iTunes store](https://itunes.apple.com/us/podcast/dataframed/id1336150688).\n",
    "* email dataframed@datacamp.com a screenshot of the review and the country in whose store you posted it (note: this email address is not regularly checked except for this give-away).\n",
    "* Do these things by EOD Friday March 2nd in your time zone.\n",
    "\n",
    "\n",
    "If you're enoying this session, retweet or share on FB now and follow us on Twitter: [@hugobowne](https://twitter.com/hugobowne) & [@DataCamp](https://twitter.com/DataCamp)."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
